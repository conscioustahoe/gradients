{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising Diffusion Probabilistic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239)\n",
    "\n",
    "[Tutorial on Diffusion Models for Imaging and Vision](https://arxiv.org/pdf/2403.18103)\n",
    "\n",
    "[Bayes theorem, the geometry of changing beliefs](https://youtu.be/HZGCoVF3YvM?si=wPw_XLl6pZFQmIws)\n",
    "\n",
    "[Probabilities of probabilities](https://youtube.com/playlist?list=PLZHQObOWTQDOjmo3Y6ADm0ScWAlEXf-fp&si=2GUpUMdHkjuCqSPK)\n",
    "\n",
    "[What are Diffusion Models?](https://youtu.be/fbLgFrlTnGU?si=_d1SbwC5wNQ6eVym)\n",
    "\n",
    "[Diffusion Models | Paper Explanation | Math Explained](https://youtu.be/HoKDTa5jHvg?si=Df3zTMRsPOgloGvI)\n",
    "\n",
    "[The Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion)\n",
    "\n",
    "[Generative Modeling by Estimating Gradients of the Data Distribution](https://yang-song.net/blog/2021/score/)\n",
    "\n",
    "[What are Diffusion Models? Author: Lilian Weng](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)\n",
    "\n",
    "[How diffusion models work: the math from scratch](https://theaisummer.com/diffusion-models/)\n",
    "\n",
    "[Step by Step visual introduction to Diffusion Models](https://erdem.pl/2023/11/step-by-step-visual-introduction-to-diffusion-models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main components of DDPM\n",
    "\n",
    "#### Forward Diffusion Process\n",
    "\n",
    "The forward diffusion process is about adding noise gradually to the data so that it eventually becomes Gaussian noise. This can be expressed as a series of transitions, where at each step, a small amount of Gaussian noise is added.\n",
    "\n",
    "#### Reverse Process (Denoising)\n",
    "\n",
    "The reverse process is more complex and involves learning how to reverse the noise addition in order to recover the original data. Here, a neural network is typically trained to predict the noise added at each step.\n",
    "\n",
    "#### Connection to Variational Inference (KL Divergence, ELBO, etc.)\n",
    "\n",
    "This part of the model helps explain why and how we can learn the reverse process. We want the reverse process to approximate the true posterior distribution of the data (i.e., the reverse distribution). This can be done by minimizing the KL divergence between the true posterior and the learned reverse process.\n",
    "\n",
    "Let’s start by walking through the **forward diffusion process**, where we gradually add noise to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Process (Diffusion of Data)\n",
    "\n",
    "In the forward process, we transform data $\\mathbf{x}_0$ (e.g., an image) into a sequence of noisy versions $\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_T$ through a gradual addition of Gaussian noise. By the end of the process (at time step $T$), the data should resemble pure noise, typically modeled as a Gaussian distribution $\\mathcal{N}(0, I)$.\n",
    "\n",
    "The forward process is defined as a Markov chain, where each state $\\mathbf{x}_t$ depends only on the previous state $\\mathbf{x}_{t-1}$. Specifically, we can express this step-wise transition as:\n",
    "\n",
    "$$\n",
    "q(\\mathbf{x}_t | \\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{1-\\beta_t} \\mathbf{x}_{t-1}, \\beta_t \\mathbf{I})\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $\\mathbf{x}_t$ is the noisy data at time step $t$.\n",
    "- $\\beta_t$ is a variance schedule controlling the amount of noise added at each step.\n",
    "- $\\mathcal{N}(\\mu, \\Sigma)$ denotes a Gaussian distribution with mean $\\mu$ and covariance $\\Sigma$.\n",
    "\n",
    "#### What's happening in each step?\n",
    "\n",
    "At each step $t$, a small amount of Gaussian noise is added to the previous state $\\mathbf{x}_{t-1}$. This noise is parameterized by $\\beta_t$, which controls how much noise is injected at each step. The forward process moves the data from its original clean state $\\mathbf{x}_0$ towards a fully noisy state $\\mathbf{x}_T$.\n",
    "\n",
    "- The term $\\sqrt{1 - \\beta_t} \\mathbf{x}_{t-1}$ scales down the contribution of the clean data.\n",
    "- The term $\\beta_t \\mathbf{I}$ introduces Gaussian noise.\n",
    "\n",
    "By the time you reach $\\mathbf{x}_T$, the data has been degraded into noise.\n",
    "\n",
    "#### How do we accumulate noise?\n",
    "\n",
    "We want to describe the relationship between $\\mathbf{x}_0$ (the original data) and $\\mathbf{x}_t$ at an arbitrary time step $t$. Since each step involves adding noise, we can compute the cumulative noise added over all the steps up to $t$.\n",
    "\n",
    "By recursively applying the noise-adding process from $t = 1$ to $t = T$, we get:\n",
    "\n",
    "$$\n",
    "q(\\mathbf{x}_t | \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0, (1 - \\bar{\\alpha}_t) \\mathbf{I})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha_t = 1 - \\beta_t$\n",
    "- $\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s$\n",
    "\n",
    "This equation tells us that $\\mathbf{x}_t$ is a noisy version of the original data $\\mathbf{x}_0$, where $\\sqrt{\\bar{\\alpha}_t}$ is a decaying factor applied to the original data, and $1 - \\bar{\\alpha}_t$ governs the amount of noise accumulated over time.\n",
    "\n",
    "#### Why is this form useful?\n",
    "\n",
    "This equation is crucial because it allows us to directly sample $\\mathbf{x}_t$ at any time step $t$ from the original data $\\mathbf{x}_0$. The cumulative noise $\\mathbf{x}_t$ is modeled as a Gaussian distribution where:\n",
    "- The mean is $\\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0$, a scaled version of the original data.\n",
    "- The variance is $(1 - \\bar{\\alpha}_t) \\mathbf{I}$, which grows over time, introducing more noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is $\\bar{\\alpha}_t$?\n",
    "\n",
    "$\\bar{\\alpha}_t$ is the cumulative product of $\\alpha_t$ values up to time step $t$. Recall that:\n",
    "\n",
    "$$\n",
    "\\alpha_t = 1 - \\beta_t\n",
    "$$\n",
    "\n",
    "where $\\beta_t$ is the variance at each time step, which controls how much noise is added at step $t$.\n",
    "\n",
    "Then, $\\bar{\\alpha}_t$ is defined as:\n",
    "\n",
    "$$\n",
    "\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s = \\prod_{s=1}^{t} (1 - \\beta_s)\n",
    "$$\n",
    "\n",
    "This represents the product of all the $\\alpha$-values up to time step $t$.\n",
    "\n",
    "#### Why does $\\bar{\\alpha}_t$ allow us to calculate the noisy data in one step?\n",
    "\n",
    "In the forward process, the goal is to add noise incrementally to $\\mathbf{x}_0$ over multiple time steps, resulting in a noisy version $\\mathbf{x}_t$ at time step $t$. Each step of the process adds some Gaussian noise, which gradually turns the original data $\\mathbf{x}_0$ into pure noise $\\mathbf{x}_T$ over $T$ steps.\n",
    "\n",
    "The forward process can be described step-by-step like this:\n",
    "\n",
    "$$\n",
    "q(\\mathbf{x}_t | \\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\alpha_t} \\mathbf{x}_{t-1}, (1-\\alpha_t) \\mathbf{I})\n",
    "$$\n",
    "\n",
    "which means:\n",
    "- $\\mathbf{x}_t$ is a Gaussian sample with a mean of $\\sqrt{\\alpha_t} \\mathbf{x}_{t-1}$ (scaled down from the previous step) and a variance of $(1-\\alpha_t) \\mathbf{I}$ (the added noise).\n",
    "\n",
    "If we were to apply this process recursively, from $t = 1$ to $t = T$, the data would become noisier and noisier. But rather than doing this recursive process manually (one step at a time), we can jump directly to any time step $t$ using the closed-form expression of $\\mathbf{x}_t$ in terms of $\\mathbf{x}_0$.\n",
    "\n",
    "This expression is:\n",
    "\n",
    "$$\n",
    "q(\\mathbf{x}_t | \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0, (1 - \\bar{\\alpha}_t) \\mathbf{I})\n",
    "$$\n",
    "\n",
    "- $\\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0$: This is the part of the original data that remains after $t$ steps. The scaling factor $\\bar{\\alpha}_t$ gets smaller over time because more noise is added, so the contribution of $\\mathbf{x}_0$ to $\\mathbf{x}_t$ diminishes.\n",
    "- $(1 - \\bar{\\alpha}_t) \\mathbf{I}$: This is the cumulative noise that has been added up to step $t$. It grows as $t$ increases, eventually turning $\\mathbf{x}_t$ into pure noise.\n",
    "\n",
    "### How does this allow for one-step calculation?\n",
    "\n",
    "Instead of iterating through each time step and adding noise gradually (which would be computationally expensive), the closed-form expression lets us compute $\\mathbf{x}_t$ directly from $\\mathbf{x}_0$ using just the cumulative term $\\bar{\\alpha}_t$.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "Imagine you start with clean data $\\mathbf{x}_0$. If you want to compute the noisy data at step $t = 100$, instead of applying noise sequentially 100 times, you can jump straight to $t = 100$ by using:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_t = \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\boldsymbol{\\epsilon}\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\epsilon}$ is standard Gaussian noise $\\mathcal{N}(0, I)$.\n",
    "\n",
    "This works because $\\bar{\\alpha}_t$ captures the total effect of the cumulative noise addition over all steps up to $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the following:\n",
    "- $\\mathbf{x}_0$ is the original data (e.g., an image).\n",
    "- The noise schedule is given by $\\beta_1, \\beta_2, \\beta_3$, and $\\alpha_t = 1 - \\beta_t$.\n",
    "- The cumulative product $\\bar{\\alpha}_t$ is $\\prod_{s=1}^{t} \\alpha_s$.\n",
    "\n",
    "### Step 1: From $t = 0$ to $t = 1$\n",
    "We start with the clean data $\\mathbf{x}_0$. The forward process at $t = 1$ is given by:\n",
    "$$\n",
    "q(\\mathbf{x}_1 | \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_1; \\sqrt{\\alpha_1} \\mathbf{x}_0, \\beta_1 \\mathbf{I})\n",
    "$$\n",
    "This means:\n",
    "- $\\mathbf{x}_1$ is a Gaussian distribution with mean $\\sqrt{\\alpha_1} \\mathbf{x}_0$ (a scaled version of the original data) and variance $\\beta_1$, which adds some Gaussian noise.\n",
    "\n",
    "The noisy data $\\mathbf{x}_1$ is a mixture of the original data $\\mathbf{x}_0$ and some added noise:\n",
    "$$\n",
    "\\mathbf{x}_1 = \\sqrt{\\alpha_1} \\mathbf{x}_0 + \\sqrt{\\beta_1} \\boldsymbol{\\epsilon}_1\n",
    "$$\n",
    "where $\\boldsymbol{\\epsilon}_1 \\sim \\mathcal{N}(0, I)$ is Gaussian noise.\n",
    "\n",
    "### Step 2: From $t = 1$ to $t = 2$\n",
    "Next, we add more noise to $\\mathbf{x}_1$. The forward process at $t = 2$ is:\n",
    "$$\n",
    "q(\\mathbf{x}_2 | \\mathbf{x}_1) = \\mathcal{N}(\\mathbf{x}_2; \\sqrt{\\alpha_2} \\mathbf{x}_1, \\beta_2 \\mathbf{I})\n",
    "$$\n",
    "Now we generate $\\mathbf{x}_2$ by adding noise to $\\mathbf{x}_1$:\n",
    "$$\n",
    "\\mathbf{x}_2 = \\sqrt{\\alpha_2} \\mathbf{x}_1 + \\sqrt{\\beta_2} \\boldsymbol{\\epsilon}_2\n",
    "$$\n",
    "Substituting $\\mathbf{x}_1$ from the previous step:\n",
    "$$\n",
    "\\mathbf{x}_2 = \\sqrt{\\alpha_2} \\left( \\sqrt{\\alpha_1} \\mathbf{x}_0 + \\sqrt{\\beta_1} \\boldsymbol{\\epsilon}_1 \\right) + \\sqrt{\\beta_2} \\boldsymbol{\\epsilon}_2\n",
    "$$\n",
    "Simplifying:\n",
    "$$\n",
    "\\mathbf{x}_2 = \\sqrt{\\alpha_2 \\alpha_1} \\mathbf{x}_0 + \\sqrt{\\alpha_2 \\beta_1} \\boldsymbol{\\epsilon}_1 + \\sqrt{\\beta_2} \\boldsymbol{\\epsilon}_2\n",
    "$$\n",
    "So, $\\mathbf{x}_2$ contains:\n",
    "- The original data $\\mathbf{x}_0$, scaled by $\\sqrt{\\alpha_2 \\alpha_1}$,\n",
    "- Noise from step 1, scaled by $\\sqrt{\\alpha_2 \\beta_1}$,\n",
    "- New noise added at step 2, $\\boldsymbol{\\epsilon}_2$, scaled by $\\sqrt{\\beta_2}$.\n",
    "\n",
    "### Step 3: From $t = 2$ to $t = 3$\n",
    "Now we repeat the process again, adding noise to $\\mathbf{x}_2$. The forward process at $t = 3$ is:\n",
    "$$\n",
    "q(\\mathbf{x}_3 | \\mathbf{x}_2) = \\mathcal{N}(\\mathbf{x}_3; \\sqrt{\\alpha_3} \\mathbf{x}_2, \\beta_3 \\mathbf{I})\n",
    "$$\n",
    "So we generate $\\mathbf{x}_3$ by:\n",
    "$$\n",
    "\\mathbf{x}_3 = \\sqrt{\\alpha_3} \\mathbf{x}_2 + \\sqrt{\\beta_3} \\boldsymbol{\\epsilon}_3\n",
    "$$\n",
    "Substitute $\\mathbf{x}_2$ from the previous step:\n",
    "$$\n",
    "\\mathbf{x}_3 = \\sqrt{\\alpha_3} \\left( \\sqrt{\\alpha_2 \\alpha_1} \\mathbf{x}_0 + \\sqrt{\\alpha_2 \\beta_1} \\boldsymbol{\\epsilon}_1 + \\sqrt{\\beta_2} \\boldsymbol{\\epsilon}_2 \\right) + \\sqrt{\\beta_3} \\boldsymbol{\\epsilon}_3\n",
    "$$\n",
    "Simplifying:\n",
    "$$\n",
    "\\mathbf{x}_3 = \\sqrt{\\alpha_3 \\alpha_2 \\alpha_1} \\mathbf{x}_0 + \\sqrt{\\alpha_3 \\alpha_2 \\beta_1} \\boldsymbol{\\epsilon}_1 + \\sqrt{\\alpha_3 \\beta_2} \\boldsymbol{\\epsilon}_2 + \\sqrt{\\beta_3} \\boldsymbol{\\epsilon}_3\n",
    "$$\n",
    "Now $\\mathbf{x}_3$ contains:\n",
    "- The original data $\\mathbf{x}_0$, scaled by $\\sqrt{\\alpha_3 \\alpha_2 \\alpha_1}$,\n",
    "- Noise added at step 1, scaled by $\\sqrt{\\alpha_3 \\alpha_2 \\beta_1}$,\n",
    "- Noise added at step 2, scaled by $\\sqrt{\\alpha_3 \\beta_2}$,\n",
    "- New noise added at step 3, $\\boldsymbol{\\epsilon}_3$, scaled by $\\sqrt{\\beta_3}$.\n",
    "\n",
    "### Using $\\bar{\\alpha}_t$ to Jump Directly from $\\mathbf{x}_0$ to $\\mathbf{x}_3$\n",
    "\n",
    "Instead of computing the noise addition step-by-step, we can jump directly to $t = 3$ using the closed-form expression with $\\bar{\\alpha}_3$.\n",
    "\n",
    "The cumulative product of the $\\alpha_t$'s up to $t = 3$ is:\n",
    "$$\n",
    "\\bar{\\alpha}_3 = \\alpha_1 \\alpha_2 \\alpha_3\n",
    "$$\n",
    "\n",
    "The formula for $\\mathbf{x}_3$ is:\n",
    "$$\n",
    "q(\\mathbf{x}_3 | \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_3; \\sqrt{\\bar{\\alpha}_3} \\mathbf{x}_0, (1 - \\bar{\\alpha}_3) \\mathbf{I})\n",
    "$$\n",
    "\n",
    "So, we can directly compute $\\mathbf{x}_3$ as:\n",
    "$$\n",
    "\\mathbf{x}_3 = \\sqrt{\\bar{\\alpha}_3} \\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_3} \\boldsymbol{\\epsilon}\n",
    "$$\n",
    "where $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, I)$.\n",
    "\n",
    "#### What does this mean?\n",
    "- Instead of calculating noise step by step from $t = 0$ to $t = 3$, we can **directly compute** $\\mathbf{x}_3$ by just applying a scaling factor to $\\mathbf{x}_0$ and adding Gaussian noise $\\boldsymbol{\\epsilon}$, with variance determined by $1 - \\bar{\\alpha}_3$.\n",
    "- $\\bar{\\alpha}_3$ encodes the cumulative scaling of the original data after 3 steps.\n",
    "- $1 - \\bar{\\alpha}_3$ represents the cumulative noise added after 3 steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reverse Process (Denoising)\n",
    "\n",
    "The reverse process is where we attempt to **undo** the forward process. While the forward process gradually turns clean data $\\mathbf{x}_0$ into noise $\\mathbf{x}_T$, the reverse process tries to reconstruct the original data $\\mathbf{x}_0$ from noisy samples $\\mathbf{x}_T$.\n",
    "\n",
    "However, we don't know the true reverse process directly (i.e., how to denoise perfectly). Instead, we **train a neural network** to learn this reverse process step-by-step by predicting the noise at each step.\n",
    "\n",
    "#### Goal of the reverse process\n",
    "We want to model the reverse transitions $p_\\theta(\\mathbf{x}_{t-1} | \\mathbf{x}_t)$, which allow us to go backward from $\\mathbf{x}_T$ to $\\mathbf{x}_0$. This is modeled as another Gaussian distribution:\n",
    "$$\n",
    "p_\\theta(\\mathbf{x}_{t-1} | \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t), \\sigma^2_t \\mathbf{I})\n",
    "$$\n",
    "Where:\n",
    "- $\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t)$ is the mean of the Gaussian distribution (which will be predicted by the neural network).\n",
    "- $\\sigma^2_t$ is the variance, which can be fixed or learned, but is often kept constant for simplicity.\n",
    "\n",
    "#### Intuition Behind the Reverse Process\n",
    "\n",
    "At each step in the reverse process, the neural network learns to predict the mean $\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t)$ — which is essentially the \"cleaner\" version of $\\mathbf{x}_t$ with some noise removed — allowing the model to step backward toward the original data $\\mathbf{x}_0$.\n",
    "\n",
    "Here's the step-by-step intuition:\n",
    "1. At $t = T$, we start with a noisy sample $\\mathbf{x}_T$ (essentially random Gaussian noise).\n",
    "2. The neural network predicts how to denoise $\\mathbf{x}_T$ into a less noisy sample $\\mathbf{x}_{T-1}$.\n",
    "3. This process is repeated in a step-by-step manner, denoising $\\mathbf{x}_t$ to obtain $\\mathbf{x}_{t-1}$, until eventually reaching $\\mathbf{x}_0$.\n",
    "\n",
    "#### Key Insight: Predicting the Noise\n",
    "\n",
    "A key insight in DDPMs is that instead of predicting $\\mathbf{x}_{t-1}$ directly, the neural network is trained to **predict the noise $\\boldsymbol{\\epsilon}_t$ added at step $t$**.\n",
    "\n",
    "Why? Because:\n",
    "- In the forward process, each $\\mathbf{x}_t$ is a combination of the clean data $\\mathbf{x}_0$ and Gaussian noise $\\boldsymbol{\\epsilon}$, as we saw earlier.\n",
    "- If the neural network can predict the noise $\\boldsymbol{\\epsilon}_t$, we can then subtract it from $\\mathbf{x}_t$ to get an estimate of the clean data.\n",
    "\n",
    "This leads to the formulation:\n",
    "$$\n",
    "\\mathbf{x}_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t) \\right) + \\sigma_t \\mathbf{z}\n",
    "$$\n",
    "Where:\n",
    "- $\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)$ is the noise predicted by the neural network at step $t$.\n",
    "- $\\mathbf{z}$ is Gaussian noise added for randomness, usually sampled from $\\mathcal{N}(0, I)$.\n",
    "- $\\sigma_t$ controls the variance in this step.\n",
    "\n",
    "#### Training the Neural Network\n",
    "\n",
    "#### Objective of Training\n",
    "The neural network is trained to minimize the difference between the **true noise $\\boldsymbol{\\epsilon}_t$** (which was added during the forward process) and the noise it **predicts $\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)$** at each time step $t$.\n",
    "\n",
    "This gives rise to the **loss function** for training the network, which is typically a simple mean squared error (MSE) between the true noise and the predicted noise:\n",
    "$$\n",
    "L_\\text{simple} = \\mathbb{E}_{\\mathbf{x}_0, t, \\boldsymbol{\\epsilon}} \\left[ \\left\\| \\boldsymbol{\\epsilon} - \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t) \\right\\|^2 \\right]\n",
    "$$\n",
    "Where:\n",
    "- $\\boldsymbol{\\epsilon}$ is the true noise sampled from $\\mathcal{N}(0, I)$ during the forward process.\n",
    "- $\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)$ is the noise predicted by the neural network at time step $t$.\n",
    "- The expectation is taken over the data $\\mathbf{x}_0$, time steps $t$, and the noise $\\boldsymbol{\\epsilon}$.\n",
    "\n",
    "#### How does training work?\n",
    "- **Input**: During training, the network is provided with noisy samples $\\mathbf{x}_t$ (generated using the forward process) and the time step $t$.\n",
    "- **Output**: The network is trained to output the noise $\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)$ that was added to generate $\\mathbf{x}_t$.\n",
    "- **Loss**: The loss function compares the predicted noise to the true noise $\\boldsymbol{\\epsilon}$, and the network adjusts its parameters to minimize this error over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import logging\n",
    "from collections.abc import Mapping\n",
    "from pathlib import Path\n",
    "from operator import attrgetter, itemgetter\n",
    "from functools import partial\n",
    "from copy import copy\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# Third-party library imports\n",
    "import fastcore.all as fc\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import random\n",
    "import gzip\n",
    "import pickle\n",
    "\n",
    "# torch imports\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn.functional as F\n",
    "from torch import tensor, nn, optim\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "from torch.nn import init\n",
    "from torch.optim import lr_scheduler\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "\n",
    "# dataset imports\n",
    "import datasets\n",
    "from datasets import load_dataset, load_dataset_builder\n",
    "\n",
    "# miniai imports\n",
    "from miniai.datasets import *\n",
    "from miniai.conv import *\n",
    "from miniai.learner import *\n",
    "from miniai.activations import *\n",
    "from miniai.init import *\n",
    "from miniai.sgd import *\n",
    "from miniai.resnet import *\n",
    "from miniai.augment import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['image.cmap'] = 'gray'\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = 'image', 'label'\n",
    "dsd = load_dataset('fashion_mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@inplace\n",
    "def transformi(b):\n",
    "    b[x] = [TF.resize(TF.to_tensor(o), (32,32)) for o in b[x]] # resize the 28x28 images to 32x32 to make it simpler for model's architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "bs = 128\n",
    "tds = dsd.with_transform(transformi)\n",
    "dls = DataLoaders.from_dd(tds, bs, num_workers=8)\n",
    "dt = dls.train\n",
    "xb,yb = next(iter(dt))\n",
    "xb.shape,yb[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
